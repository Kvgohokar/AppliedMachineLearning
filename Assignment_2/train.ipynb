{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Experiment Tracking\n",
    "\n",
    "## 2. Model Version Control and Experiment Tracking\n",
    "\n",
    "### Track Experiments and Model Versions using MLflow\n",
    "\n",
    "In `train.ipynb`, track the experiments and model versions using **MLflow**:\n",
    "\n",
    "1. **Build, track, and register 3 benchmark models** using MLflow.\n",
    "2. Checkout and print the model selection metric **AUCPR** for each of the three benchmark models.\n",
    "\n",
    "#### References for ML Experiment Tracking\n",
    "\n",
    "- [MLflow Tracking Documentation](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLflow Quickstart Guide](https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html)\n",
    "- [Tracking ML Experiments with MLflow](https://www.datarevenue.com/en-blog/how-we-track-machine-learning-experiments-with-mlflow)\n",
    "- [Experiment Tracking with MLflow](https://towardsdatascience.com/experiment-tracking-with-mlflow-in-10-minutes-f7c2128b8f2c)\n",
    "- [MadeWithML - Experiment Tracking](https://madewithml.com/courses/mlops/experiment-tracking/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score,roc_curve, auc, accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "val_data = pd.read_csv(\"./val.csv\")\n",
    "test_data = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message    5\n",
      "label      0\n",
      "dtype: int64 message    0\n",
      "label      0\n",
      "dtype: int64 message    1\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum(), val_data.isnull().sum(), test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaNs in any column\n",
    "train_data = train_data.dropna()\n",
    "val_data = val_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "X_train, y_train = train_data.drop(columns=\"label\"), train_data[\"label\"].values\n",
    "X_val, y_val = val_data.drop(columns=\"label\"), val_data[\"label\"].values\n",
    "X_test, y_test = test_data.drop(columns=\"label\"), test_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = vectorizer.fit_transform(X_train['message'])\n",
    "\n",
    "# Transform the validation and test data\n",
    "X_val = vectorizer.transform(X_val['message'])\n",
    "X_test = vectorizer.transform(X_test['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Best Params: {'C': 10, 'penalty': 'l2'}\n",
      "Validation Accuracy: 0.9709\n",
      "AUC ROC Score: 0.9794\n",
      "AUCPR Score: 0.9519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Logistic Regression'.\n",
      "Created version '1' of model 'Logistic Regression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression logged and registered in MLflow.\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Best Params: {'max_depth': None, 'n_estimators': 200}\n",
      "Validation Accuracy: 0.9596\n",
      "AUC ROC Score: 0.9908\n",
      "AUCPR Score: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Random Forest'.\n",
      "Created version '1' of model 'Random Forest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest logged and registered in MLflow.\n",
      "\n",
      "\n",
      "Training SVM...\n",
      "Best Params: {'C': 10, 'kernel': 'linear'}\n",
      "Validation Accuracy: 0.9798\n",
      "AUC ROC Score: 0.9774\n",
      "AUCPR Score: 0.9524\n",
      "SVM logged and registered in MLflow.\n",
      "\n",
      "All models trained and logged in MLflow successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'SVM'.\n",
      "Created version '1' of model 'SVM'.\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"Benchmark_Models\")\n",
    "input_example = X_test[:1].toarray() \n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"param_grid\": {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']},\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"param_grid\": {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, None]},\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"param_grid\": {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    },\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "\n",
    "for model_name, details in models.items():\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "\n",
    "        # Grid Search for Hyperparameter tuning\n",
    "        search = GridSearchCV(details[\"model\"], details[\"param_grid\"], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        search.fit(X_train, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        # Validation Accuracy\n",
    "        val_accuracy = accuracy_score(y_val, best_model.predict(X_val))\n",
    "        \n",
    "        # Test predictions\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # AUC ROC Score\n",
    "        roc_auc = roc_auc_score(y_test, y_test_prob, multi_class='ovr')\n",
    "        \n",
    "        # AUCPR Score\n",
    "        aucpr = average_precision_score(y_test, y_test_prob)\n",
    "        \n",
    "        print(f\"Best Params: {search.best_params_}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"AUC ROC Score: {roc_auc:.4f}\")\n",
    "        print(f\"AUCPR Score: {aucpr:.4f}\")\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_params(search.best_params_)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"aucpr\", aucpr)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(sk_model=best_model,artifact_path=model_name,input_example=input_example)\n",
    "        \n",
    "        # Register best model\n",
    "        mlflow.register_model(f\"runs:/{mlflow.active_run().info.run_id}/{model_name}\", model_name)\n",
    "\n",
    "        print(f\"{model_name} logged and registered in MLflow.\\n\")\n",
    "\n",
    "print(\"All models trained and logged in MLflow successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
